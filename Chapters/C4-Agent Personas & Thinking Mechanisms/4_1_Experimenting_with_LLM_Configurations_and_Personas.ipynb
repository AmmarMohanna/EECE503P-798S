{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Lab 1: Exploring LLM Behavior with Configuration and Persona Experiments"
      ],
      "metadata": {
        "id": "OFKKEYoFwNZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this lab, you’ll **experiment with generation parameters** and **system personas** to understand how they influence the behavior of a Large Language Model. You'll run controlled experiments to see how the model's tone, creativity, and coherence shift with different configurations\n",
        "\n",
        "What You'll Learn:\n",
        "\n",
        "* How to **tune generation parameters** like `temperature` and `top_p` to control randomness and diversity\n",
        "* How to **design system personas** to shape the model’s writing style and point of view\n",
        "* How to run **side-by-side comparisons** of model outputs across multiple settings\n",
        "* How to apply this knowledge to **creative writing, technical answers, or branding consistency**\n",
        "* How to structure simple experiments for **LLM behavior profiling**\n",
        "\n",
        "This lab will give you practical insight into **how LLMs think** — and how to steer their thinking to fit your goals.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "YQ-i2WGnw3I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "You can use:\n",
        "* OpenAI (GPT-4, GPT-3.5)\n",
        "* Anthropic (Claude)\n",
        "* Google (Gemini)\n",
        "* Local (via Ollama or LM Studio: Mistral, LLaMA, etc.)\n"
      ],
      "metadata": {
        "id": "DSe5rUM7xGfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "google_api_key = '' #add api key\n",
        "client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
        "model_name = \"gemini-2.0-flash\""
      ],
      "metadata": {
        "id": "ztc9L9ugx0d8"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Temperature and max_tokens\n",
        "\n",
        "This experiment investigates how the temperature parameter influences the creativity and randomness of the model's responses.The max_tokens parameter is set to 100 to limit the length of the responses but you can change as you'd like.\n",
        "\n",
        "* **Temperature 0.1:** Produces more focused and less speculative output.\n",
        "* **Temperature 0.5:** Generates moderately varied and imaginative descriptions.\n",
        "* **Temperature 0.9:** Results in highly creative and diverse responses."
      ],
      "metadata": {
        "id": "J6Tb1xWYyCaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt to test\n",
        "prompt =\"Describe what the inside of a superintelligent AI's dream might look like.\"\n",
        "\n",
        "# Temperatures to test\n",
        "temperatures = [0.1, 0.5, 0.9]\n",
        "\n",
        "# Loop through temperatures and print results\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n Temperature: {temp}\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temp,\n",
        "        max_tokens=100,\n",
        "        n=1,\n",
        "    )\n",
        "    print(response.choices[0].message.content.strip())"
      ],
      "metadata": {
        "id": "9uNbe3HAiocZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Top P\n",
        "This section explores the effect of the top_p parameter, which controls the diversity of the output by sampling from the most probable tokens whose cumulative probability exceeds the top_p value. The temperature is fixed at 0.9 for this experiment.\n",
        "\n",
        "* **top_p 0.3:** Leads to more constrained and less varied responses.\n",
        "* **top_p 0.7:** Offers a balance between focus and diversity.\n",
        "* **top_p 1.0:** Generates highly diverse and imaginative descriptions"
      ],
      "metadata": {
        "id": "-WLmumSinDOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt to test\n",
        "prompt = \"Describe what the inside of a superintelligent AI's dream might look like.\"\n",
        "\n",
        "# Temperatures and top_p values to test\n",
        "temperatures = 0.5\n",
        "top_ps = [0.3, 0.7, 1.0]\n",
        "\n",
        "# Loop through temperature and top_p combinations\n",
        "for top_p in top_ps:\n",
        "        print(f\"\\n=== Temperature: {temp} | Top-p: {top_p} ===\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=temp,\n",
        "            top_p=top_p,\n",
        "            max_tokens=150,\n",
        "            n=1,\n",
        "        )\n",
        "        print(response.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "id": "88x8GP5jivtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Persona Experiment\n",
        "Using the prompt:\n",
        "“Explain how a black hole forms”,\n",
        "we apply different system-level personas to see how the tone and depth vary."
      ],
      "metadata": {
        "id": "VO4MVpVDysfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "personas = {\n",
        "    \"expert\": \"You are an expert who gives detailed, precise, and factual answers.\",\n",
        "    \"creative\": \"You are a creative storyteller who thinks outside the box.\",\n",
        "    \"minimal\": \"You give short and concise answers.\",\n",
        "    \"friendly_teacher\": \"You are a friendly teacher who explains things simply and clearly.\"\n",
        "}\n",
        "\n",
        "prompt = \"Explain how a black hole forms.\"\n"
      ],
      "metadata": {
        "id": "y1cB-D1poyBq"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_persona(prompt, persona, max_tokens=150):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": persona},  # <-- Persona here\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        n=1,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n"
      ],
      "metadata": {
        "id": "M2e8D3tWo0s6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, persona_prompt in personas.items():\n",
        "    print(f\"\\n--- Persona: {name} ---\")\n",
        "    output = query_persona(prompt, persona_prompt)\n",
        "    print(output)"
      ],
      "metadata": {
        "id": "2az8h_S9pBaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations:**\n",
        "* **Expert:** Dense with facts and jargon\n",
        "* **Creative:** Uses metaphors and vivid imagery\n",
        "* **Minimal:** One-liner response\n",
        "* **Friendly Teacher:** Accessible explanations with analogies"
      ],
      "metadata": {
        "id": "k16jxhqpy1Xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Persona × Temperature Combined:\n",
        "\n",
        "Let’s combine a well-defined persona with varying temperature values to observe how the model’s style, focus, and creativity shift across the spectrum."
      ],
      "metadata": {
        "id": "_2G9tg_-zFcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# Creative persona prompt\n",
        "persona = (\n",
        "    \"You are a professional blog post writer who creates clear, engaging, and well-structured content. \"\n",
        "    \"You write in a friendly and approachable tone, use vivid examples, and explain complex ideas simply. \"\n",
        "    \"Your writing is informative yet accessible, with smooth transitions and a natural flow. \"\n",
        "    \"Make sure to captivate the reader’s interest from the start and provide insightful perspectives on the topic.\"\n",
        ")\n",
        "\n",
        "\n",
        "# Prompt to test\n",
        "prompt = \"Talk about AI\"\n",
        "\n",
        "# Temperatures to try\n",
        "temperatures = [0.1, 0.5, 0.9]\n",
        "\n",
        "def query_model(prompt, persona, temperature=0.7, max_tokens=150):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": persona},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        n=1\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Run the experiment for different temperatures\n",
        "for temp in temperatures:\n",
        "    print(f\"\\n--- Temperature: {temp} ---\")\n",
        "    output = query_model(prompt, persona, temperature=temp)\n",
        "    print(output)\n"
      ],
      "metadata": {
        "id": "3Xziz9D-uUFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion:\n",
        "\n",
        "* **Temperature** controls the randomness and risk-taking of the model.\n",
        "* **Top-p** adjusts how much of the probability distribution is considered.\n",
        "* **System messages (personas)** strongly influence tone, voice, and format.\n",
        "\n",
        "## Next Steps:\n",
        "* Try prompts in other domains (e.g., math, philosophy, humor)\n",
        "* Use multiple personas per session and different configurations\n",
        "* Evaluate coherence with LLM-as-judge or heuristics"
      ],
      "metadata": {
        "id": "l2se2JFnziZv"
      }
    }
  ]
}