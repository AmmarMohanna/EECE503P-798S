{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 2: Exploring and Comparing Large Language Model Thinking Strategies"
      ],
      "metadata": {
        "id": "5YagMkPvgtJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we explore different reasoning and thinking mechanisms employed by large language models (LLMs) to solve problems and answer questions. These mechanisms—including zero-shot prompting, chain-of-thought (CoT), and tree-of-thought (ToT) reasoning—shape how the model processes information and reaches conclusions.\n"
      ],
      "metadata": {
        "id": "MwojnHqQgs4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thoughts"
      ],
      "metadata": {
        "id": "lvVMlsozfPiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chain-of-Thought (CoT) prompting guides large language models to solve problems by breaking them down into intermediate, logical steps, much like showing one's work in mathematics. This section explores three variations of Chain-of-Thought (CoT) prompting:\n",
        "\n",
        "* **zero_shot:** This is a basic prompt where the model is simply asked the question without any explicit instructions to think step-by-step or provide examples. The expectation is that the model will directly provide the answer.\n",
        "\n",
        "* **zero_shot_cot:** This technique adds a phrase like \"Let's think step by step.\" to the prompt, encouraging the model to break down the problem and show its reasoning process before giving the final answer. This is often effective in improving accuracy for complex reasoning tasks.\n",
        "\n",
        "* **manual_cot:** This method involves providing the model with a few-shot examples where both the question and the detailed, step-by-step reasoning (the \"chain of thought\") leading to the answer are explicitly shown. This trains the model to emulate the provided reasoning style for new, similar questions"
      ],
      "metadata": {
        "id": "hMpzs0joezOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "def run_model(prompt: str, model: str = \"gemini-2.0-flash\", temperature: float = 0.7) -> str:\n",
        "\n",
        "    google_api_key = 'AIzaSyBn6Tw1tlEpOMLSIR1KhrW52V_7XlBzXco'  # <- Add your Gemini API Key here\n",
        "    client = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n"
      ],
      "metadata": {
        "id": "44nd_AbhE9OS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZySH19it_Nia"
      },
      "outputs": [],
      "source": [
        "def build_cot_prompt(method: str, question: str) -> str:\n",
        "    if method == \"zero_shot\":\n",
        "        return f\"Q: {question}\\nA: The answer is\"\n",
        "\n",
        "    elif method == \"zero_shot_cot\":\n",
        "        return f\"Q: {question}\\nA: Let's think step by step.\"\n",
        "\n",
        "    elif method == \"manual_cot\":\n",
        "        few_shot_examples = \"\"\"\n",
        "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
        "A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.\n",
        "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
        "A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\n",
        "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.\n",
        "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?\n",
        "A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.\n",
        "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?\n",
        "A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.\n",
        "\"\"\"\n",
        "        return few_shot_examples + f\"\\nQ: {question}\\nA:\"\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid method\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "method = \"manual_cot\"  # Can be: \"zero_shot\", \"zero_shot_cot\", or \"manual_cot\"\n",
        "model = \"gemini-2.0-flash\"\n",
        "\n",
        "question = \"There were 10 friends playing a video game online when 7 players quit. If each player left had 8 lives, how many lives did they have total?\"\n",
        "\n",
        "print(\"*****************************\")\n",
        "print(\"Test Question:\")\n",
        "print(question)\n",
        "print(\"*****************************\")\n",
        "\n",
        "prompt = build_cot_prompt(method, question)\n",
        "print(\"Prompted Input:\")\n",
        "print(prompt)\n",
        "print(\"*****************************\")\n",
        "\n",
        "output = run_model(prompt, model=model)\n",
        "print(\"Output:\")\n",
        "print(output)\n",
        "print(\"*****************************\")\n"
      ],
      "metadata": {
        "id": "YE8rZtcMEcJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tree of Thought"
      ],
      "metadata": {
        "id": "uH2eX3rHdfo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tree of Thought (ToT) is a more advanced technique that simulates a collaborative problem-solving process, where multiple \"experts\" (different reasoning paths within the model) contribute and refine their thinking in steps, potentially leading to more robust and accurate solutions."
      ],
      "metadata": {
        "id": "6gLf0qq3fjcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Bob is in the living room.\n",
        "He walks to the kitchen, carrying a cup.\n",
        "He puts a ball in the cup and carries the cup to the bedroom.\n",
        "He turns the cup upside down, then walks to the garden.\n",
        "He puts the cup down in the garden, then walks to the garage.\n",
        "Where is the ball?\"\"\"\n",
        "\n",
        "\n",
        "print(\"*****************************\")\n",
        "print(\"Test Question:\")\n",
        "print(question)\n",
        "print(\"*****************************\")\n",
        "\n",
        "print(\"*****************************\")\n",
        "prompt = build_cot_prompt(method='zero_shot',question=question)\n",
        "print(\"Prompted Input:\")\n",
        "print(prompt)\n",
        "print(\"*****************************\")\n",
        "\n",
        "\n",
        "output = run_model(prompt, model=model)\n",
        "print(\"Output:\")\n",
        "print(output)\n",
        "print(\"*****************************\")\n"
      ],
      "metadata": {
        "id": "VcOAYXxhdNhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Bob is in the living room.\n",
        "He walks to the kitchen, carrying a cup.\n",
        "He puts a ball in the cup and carries the cup to the bedroom.\n",
        "He turns the cup upside down, then walks to the garden.\n",
        "He puts the cup down in the garden, then walks to the garage.\n",
        "Where is the ball?\"\"\"\n",
        "\n",
        "system_prompt = f\"\"\"Imagine three different experts are answering this question.\n",
        "All experts will write down 1 step of their thinking,\n",
        "then share it with the group.\n",
        "Then all experts will go on to the next step, etc.\n",
        "If any expert realises they're wrong at any point then they leave.\n",
        "The question is: {question}\"\"\"\n",
        "\n",
        "print(\"*****************************\")\n",
        "print(\"Test Question:\")\n",
        "print(question)\n",
        "print(\"*****************************\")\n",
        "\n",
        "print(\"*****************************\")\n",
        "print(\"Prompt:\")\n",
        "print(system_prompt)\n",
        "print(\"*****************************\")\n",
        "\n",
        "output = run_model(system_prompt, model=model)\n",
        "print(\"Output:\")\n",
        "print(output)\n",
        "print(\"*****************************\")\n"
      ],
      "metadata": {
        "id": "USFabPAuZczk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated how different thinking mechanisms—zero-shot prompting, chain-of-thought (CoT), and tree-of-thought (ToT)—influence large language models’ reasoning and problem-solving abilities.\n",
        "\n",
        "* **Zero-shot** approaches provide quick answers but often lack detailed explanation or intermediate reasoning.\n",
        "* **CoT** improves transparency and accuracy by encouraging step-by-step reasoning\n",
        "* **ToT** further enhances problem-solving by exploring multiple reasoning paths, enabling the model to evaluate alternatives before arriving at a conclusion.\n",
        "\n",
        "Try different questions and problem types to discover which thinking mechanism best suits each use case and yields the most accurate and insightful responses.\n",
        "\n"
      ],
      "metadata": {
        "id": "14TEdslDhAUf"
      }
    }
  ]
}