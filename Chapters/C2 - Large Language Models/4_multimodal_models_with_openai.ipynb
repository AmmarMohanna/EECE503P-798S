{"cells":[{"cell_type":"markdown","id":"ca6df797","metadata":{"id":"ca6df797"},"source":["# Multimodal Inputs with GPT-4o\n","\n","This notebook shows how to attach images, audio, and documents when calling GPT-4o using the OpenAI Python SDK."]},{"cell_type":"code","source":["!pip install openai --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":561},"collapsed":true,"id":"o03Ijvo-IQpQ","executionInfo":{"status":"ok","timestamp":1752516620920,"user_tz":-180,"elapsed":10917,"user":{"displayName":"Jana Kibrit","userId":"13986201213012064329"}},"outputId":"a55745bb-27a2-4d70-a19d-126398607fb1"},"id":"o03Ijvo-IQpQ","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.94.0)\n","Collecting openai\n","  Downloading openai-1.95.1-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n","Downloading openai-1.95.1-py3-none-any.whl (755 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.94.0\n","    Uninstalling openai-1.94.0:\n","      Successfully uninstalled openai-1.94.0\n","Successfully installed openai-1.95.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["openai"]},"id":"5f5fe4bc541c4e499cb9096eb908cedc"}},"metadata":{}}]},{"cell_type":"code","execution_count":7,"id":"bca5fb45","metadata":{"id":"bca5fb45","executionInfo":{"status":"ok","timestamp":1752517411195,"user_tz":-180,"elapsed":685,"user":{"displayName":"Jana Kibrit","userId":"13986201213012064329"}}},"outputs":[],"source":["import openai\n","import base64\n","from openai import OpenAI\n","from google.colab import userdata\n","\n","openai_api_key = userdata.get('OPENAI_API_KEY')\n","client = OpenAI(api_key=openai_api_key)"]},{"cell_type":"markdown","id":"4e6d8841","metadata":{"id":"4e6d8841"},"source":["## 1. Sending an Image\n","Upload an image file and send it to GPT-4o."]},{"cell_type":"code","source":["# Function to encode the image\n","def encode_image(image_path):\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n","\n","\n","# Path to your image\n","image_path = \"/content/cat.jpg\"\n","\n","# Getting the Base64 string\n","base64_image = encode_image(image_path)\n","\n","\n","response = client.responses.create(\n","    model=\"gpt-4.1\",\n","    input=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n","                {\n","                    \"type\": \"input_image\",\n","                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n","                },\n","            ],\n","        }\n","    ],\n",")\n","\n","print(response.output_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWYdV0nyJ1JH","executionInfo":{"status":"ok","timestamp":1752517423147,"user_tz":-180,"elapsed":2865,"user":{"displayName":"Jana Kibrit","userId":"13986201213012064329"}},"outputId":"d07503ad-e602-4a4e-f284-b8ff11223062"},"id":"jWYdV0nyJ1JH","execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["This image shows a cute, fluffy kitten with tabby markings. The kitten has blue eyes and is lying on a soft, white surface, looking directly at the camera. The background is blurred, drawing focus to the kitten.\n"]}]},{"cell_type":"code","source":["response = client.responses.create(\n","    model=\"gpt-4.1-mini\",\n","    input=[{\n","        \"role\": \"user\",\n","        \"content\": [\n","            {\"type\": \"input_text\", \"text\": \"what's in this image?\"},\n","            {\n","                \"type\": \"input_image\",\n","                \"image_url\": \"https://www.hartz.com/wp-content/uploads/2022/04/small-dog-owners-1.jpg\",\n","            },\n","        ],\n","    }],\n",")\n","\n","print(response.output_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CB-uZcG0KkFT","executionInfo":{"status":"ok","timestamp":1752517518314,"user_tz":-180,"elapsed":2854,"user":{"displayName":"Jana Kibrit","userId":"13986201213012064329"}},"outputId":"916497e7-fc57-4735-be73-b9a55f815771"},"id":"CB-uZcG0KkFT","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["This image shows a close-up of a small dog, likely a Yorkshire Terrier. The dog has light brown and tan fur with a slightly darker brown beard around its mouth. The background is blurred greenery, suggesting the photo was taken outdoors. The dog is wearing a collar.\n"]}]},{"cell_type":"markdown","id":"7ffdf030","metadata":{"id":"7ffdf030"},"source":["## 2. Sending an Audio Clip\n","Upload an audio file and send it to GPT-4o."]},{"cell_type":"code","execution_count":12,"id":"39149548","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"39149548","executionInfo":{"status":"ok","timestamp":1752518226675,"user_tz":-180,"elapsed":2008,"user":{"displayName":"Jana Kibrit","userId":"13986201213012064329"}},"outputId":"f4bd20b7-745b-4e72-fab6-02aa0bd071b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["The beach, the swimsuit, the swimming trunks, the sandals, the air mattress, the towel, the ice cream, the ball, the sun, the sea, the waves,\n"]}],"source":["audio_file = open(\"/content/beach-german.mp3\", \"rb\")\n","\n","translation = client.audio.translations.create(\n","    model=\"whisper-1\",\n","    file=audio_file,\n",")\n","\n","print(translation.text)"]},{"cell_type":"markdown","id":"32bb5304","metadata":{"id":"32bb5304"},"source":["## 3. Sending a Document (PDF)\n","Upload a PDF file and send it to GPT-4o."]},{"cell_type":"code","execution_count":14,"id":"3bc5943b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3bc5943b","executionInfo":{"status":"ok","timestamp":1752518538725,"user_tz":-180,"elapsed":21325,"user":{"displayName":"Jana Kibrit","userId":"13986201213012064329"}},"outputId":"3bc8438f-ab8e-47f5-b711-894d38a8706c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Certainly! The text you provided is an excerpt from the influential paper **\"Attention Is All You Need\"** by Vaswani et al., which introduced the **Transformer** architecture.\n","\n","Below is a summary of the **key points**:\n","\n","---\n","\n","### 1. **Permission and Attribution**\n","- Google grants permission to reproduce tables and figures from the paper in journalistic or scholarly works, given proper attribution.\n","\n","---\n","\n","### 2. **Abstract and Introduction**\n","- Traditional sequence transduction models use complex recurrent neural networks (RNNs) or convolutional neural networks (CNNs) with attention mechanisms.\n","- The authors propose the **Transformer**, a new architecture that relies solely on *attention mechanisms*, eliminating recurrence and convolutions entirely.\n","- On machine translation tasks, Transformers outperform previous models in both quality and training efficiency.\n","- The Transformer achieved state-of-the-art results in multiple benchmarks: WMT 2014 English-German (28.4 BLEU) and English-French (41.8 BLEU).\n","- Transformer is easily parallelizable and trains significantly faster than previous architectures.\n","\n","---\n","\n","### 3. **Background**\n","- Previous attempts at reducing sequential computation included CNN-based models (Extended Neural GPU, ByteNet, ConvS2S), but they struggle with capturing distant dependencies efficiently.\n","- Self-attention mechanisms (intra-attention) have shown promise across many NLP tasks.\n","- The Transformer is the first architecture to rely entirely on self-attention, with no recurrence or convolution.\n","\n","---\n","\n","### 4. **Model Architecture**\n","- **Encoder-Decoder Structure**: Both encoder and decoder are stacks of identical layers (N = 6).\n","    - Each encoder layer has:\n","        - Multi-head self-attention.\n","        - Feed-forward network.\n","    - Each decoder layer adds:\n","        - Multi-head self-attention.\n","        - Multi-head encoder-decoder attention.\n","        - Feed-forward network.\n","        - Masking to prevent future token attention (for auto-regressive generation).\n","    - Layer normalization and residual connections are used.\n","- **Attention Mechanisms**:\n","    - **Scaled Dot-Product Attention**: Computes a weighted sum of values, scaling dot-products of queries and keys.\n","    - **Multi-Head Attention**: Allows the model to jointly attend to information from different representation spaces.\n","- **Feed-Forward Networks** are applied point-wise after attention.\n","- **Embeddings & Positional Encoding**: Since there's no recurrence/convolution, positional encodings (sinusoids) are added to token embeddings to inject sequence order.\n","\n","---\n","\n","### 5. **Advantages of Self-Attention**\n","- **Computational Complexity**: Self-attention allows for shorter path lengths between dependencies, faster computation (especially for shorter sequences).\n","- **Parallelization**: Entire sequences can be processed in parallel, contrary to RNNs.\n","\n","---\n","\n","### 6. **Training Details**\n","- Trained on WMT 2014 English-German (4.5M pairs) and English-French (36M pairs) datasets.\n","- Uses Adam optimizer with learning rate schedule and label smoothing regularization.\n","- Dropout is applied for regularization.\n","\n","---\n","\n","### 7. **Results**\n","- **Performance**: Transformer outperforms previous models in both BLEU score and efficiency.\n","    - Base model outperforms previous ensembles at a fraction of the training cost.\n","    - Big model achieves new state-of-the-art.\n","- **Ablation Studies**: The paper investigates the effects of changing model size, attention head count, and positional encoding.\n","- **Generalization**: The Transformer performs well on other tasks, e.g., English constituency parsing, further demonstrating flexibility.\n","\n","---\n","\n","### 8. **Conclusion**\n","- **Transformer** is the first transduction model based solely on attention, massively reducing training time and improving translation quality.\n","- The architecture is promising for other modalities and tasks beyond text.\n","\n","---\n","\n","### 9. **Additional Features**\n","- The paper provides visualizations showing how attention heads capture structural, syntactic, and semantic relationships.\n","- References and detailed explanations are provided for replication and future exploration.\n","\n","---\n","\n","## **In Summary**\n","**\"Attention Is All You Need\"** introduces the Transformer, a novel architecture that:\n","- Relies exclusively on attention mechanisms.\n","- Discards recurrence and convolution.\n","- Attains state-of-the-art results on translation tasks.\n","- Is faster, more parallelizable, and more generalizable than prior models.\n","This work laid the foundation for modern large-scale language models (like BERT and GPT).\n"]}],"source":["response = client.responses.create(\n","    model=\"gpt-4.1\",\n","    input=[\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\n","                    \"type\": \"input_file\",\n","                    \"file_url\": \"https://arxiv.org/pdf/1706.03762\",\n","                },\n","                {\n","                    \"type\": \"input_text\",\n","                    \"text\": \"Analyze the letter and provide a summary of the key points.\",\n","                },\n","            ],\n","        },\n","    ]\n",")\n","\n","print(response.output_text)"]},{"cell_type":"code","source":[],"metadata":{"id":"Wm-sql_CPDqW"},"id":"Wm-sql_CPDqW","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}