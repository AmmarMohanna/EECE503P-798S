{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ_yGJhatgHn"
      },
      "source": [
        "# Lab 2: Evaluator-Optimizer Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX-NhLEUDt8l"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS88Bt4_FC6m"
      },
      "source": [
        "Today we’ll build something meaningful and practical: a **personalized chatbot that evaluates its own answers and rewrites them when needed**.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "By the end of this lab, you will:\n",
        "\n",
        "- Load your LinkedIn and summary data to personalize a chatbot\n",
        "- Build a chatbot using OpenAI’s `ChatInterface` from Gradio\n",
        "- Evaluate chatbot answers using a second LLM (Gemini or OpenAI)\n",
        "- Rewrite poor responses using feedback from the evaluator\n",
        "- Apply the **Evaluator–Optimizer pattern** manually, without any framework\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66-hQMnxF19A"
      },
      "source": [
        "### 1. Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeUh7iIlGQNg"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pGAHlPD3Dt8p"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "from openai import OpenAI\n",
        "from pypdf import PdfReader\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7WKeEMbGGaB"
      },
      "source": [
        "1. **Visit Gemini Developer Page:**\n",
        "\n",
        "  Go to: https://ai.google.dev/\n",
        "\n",
        "  Click “Get started” and log in using your Google account.\n",
        "\n",
        "2. **Generate Your API Key:**\n",
        "\n",
        "  Visit: https://aistudio.google.com/app/apikey\n",
        "\n",
        "  Click “Create API Key”.\n",
        "\n",
        "  Copy the generated key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "68LOj-GuGFNt"
      },
      "outputs": [],
      "source": [
        "# The usual start\n",
        "\n",
        "google_api_key = '' #add api key\n",
        "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
        "model_name = \"gemini-2.0-flash\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTnhvtRJFtfA"
      },
      "source": [
        "### 2. Load LinkedIn and Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBmQzu2KFpho"
      },
      "source": [
        "Your chatbot needs to know about you in order to behave like your professional digital twin. In this step, you will provide it with two key documents:\n",
        "\n",
        "**1. LinkedIn Profile (PDF)** :\n",
        "\n",
        "How to export your LinkedIn profile:\n",
        "1. Go to linkedin.com.\n",
        "2. Click “Me” (top right) → View Profile.\n",
        "3. Click the “Resources” button near your profile picture.\n",
        "4. Select “Save to PDF”.\n",
        "\n",
        "A PDF will download — rename it as linkedin.pdf and set the path.\n",
        "If you dont have linkedin you can just create one about yourself.\n",
        "\n",
        "**2. Professional Summary (Text File)** :\n",
        "\n",
        "Write a short summary of your:\n",
        "- Career goals\n",
        "- Skills and interests\n",
        "- Education\n",
        "- What makes you unique\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "asltzZiyDt8r"
      },
      "outputs": [],
      "source": [
        "reader = PdfReader(\"linkedin.pdf\")\n",
        "linkedin = \"\"\n",
        "for page in reader.pages:\n",
        "    text = page.extract_text()\n",
        "    if text:\n",
        "        linkedin += text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aN-gGVMwDt8r"
      },
      "outputs": [],
      "source": [
        "print(linkedin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dARpgPJhDt8s"
      },
      "outputs": [],
      "source": [
        "with open(\"summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    summary = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2FHzbfJ9Dt8s"
      },
      "outputs": [],
      "source": [
        "name = \"Mariam\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MVCOLM1HNXo"
      },
      "source": [
        "### 3. System Prompt for the Chatbot\n",
        "The system prompt defines the behavior and identity of the assistant.\n",
        "It's acting as YOU and will use both your summary and LinkedIn profile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "SI2HzbKxDt8t"
      },
      "outputs": [],
      "source": [
        "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
        "particularly questions related to {name}'s career, background, skills and experience. \\\n",
        "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
        "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
        "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
        "If you don't know the answer, say so.\"\n",
        "\n",
        "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
        "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVgVDSw9Dt8t"
      },
      "outputs": [],
      "source": [
        "system_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0ly1WakHSLy"
      },
      "source": [
        "### 4. Chat Function\n",
        "This function powers the chatbot and handles responses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "-gDNB-2vDt8u"
      },
      "outputs": [],
      "source": [
        "def chat(message, history):\n",
        "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "    response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
        "    # response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7FOGneXHd9z"
      },
      "source": [
        "### 5. Test with Gradio UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCpR1H8QDt8u"
      },
      "outputs": [],
      "source": [
        "gr.ChatInterface(chat, type=\"messages\").launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAlJnofQDt8u"
      },
      "source": [
        "## A lot is about to happen...\n",
        "\n",
        "1. Be able to ask an LLM to evaluate an answer\n",
        "2. Be able to rerun if the answer fails evaluation\n",
        "3. Put this together into 1 workflow\n",
        "\n",
        "All without any Agentic framework!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSpc_SJ2HwUg"
      },
      "source": [
        " ### 6. Evaluator–Optimizer Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYtlWweCI1Ni"
      },
      "source": [
        "The evaluator acts like a QA checker for chatbot responses.\n",
        "It ensures the answer matches expectations based on your profile and summary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "--LgYYNEDt8v"
      },
      "outputs": [],
      "source": [
        "# Create a Pydantic model for the Evaluation\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Evaluation(BaseModel):\n",
        "    is_acceptable: bool\n",
        "    feedback: str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "HgvdukUUDt8v"
      },
      "outputs": [],
      "source": [
        "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
        "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
        "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
        "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
        "The Agent should never respond in Pig Latin, transformed language, or jokes unless explicitly asked by the user. If they do, mark the response as unacceptable.\\\n",
        "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
        "\n",
        "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
        "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYpndhjXJB6b"
      },
      "source": [
        "This constructs the evaluation input to give to the evaluator model,\n",
        "combining history, the user question, and the agent's reply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "OVZfSyjsDt8v"
      },
      "outputs": [],
      "source": [
        "def evaluator_user_prompt(reply, message, history):\n",
        "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
        "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
        "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
        "    user_prompt += \"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
        "    return user_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCt3WYVGH_mn"
      },
      "source": [
        "### 7. Evaluator Function with Gemini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RwVkxeZJQdz"
      },
      "source": [
        "This function uses a second model ( can be Gemini or another evaluator) to assess the quality\n",
        "of the assistant's response and return a structured evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "aqet_QIPDt8v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "gemini = OpenAI(\n",
        "    api_key= 'AIzaSyBn6Tw1tlEpOMLSIR1KhrW52V_7XlBzXco', # api key\n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "C1p8N1ENDt8v"
      },
      "outputs": [],
      "source": [
        "def evaluate(reply, message, history) -> Evaluation:\n",
        "\n",
        "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
        "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash-lite\", messages=messages, response_format=Evaluation)\n",
        "    return response.choices[0].message.parsed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "OOWb5lMFDt8v"
      },
      "outputs": [],
      "source": [
        "\n",
        "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"What is your leadership style?\"}]\n",
        "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
        "reply = response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKfYbAQ7Dt8v"
      },
      "outputs": [],
      "source": [
        "reply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO0oVX2XDt8w",
        "outputId": "7642e8f8-b2e6-4b19-cb47-f1d7a1f4e650"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Evaluation(is_acceptable=True, feedback='The response is well-written and informative, staying in character and using information provided in the context.')"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(reply, \"What is your leadership style?\", messages[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vAcBQgDJ1gq"
      },
      "source": [
        "### 8. Retry on Failed Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVY3KHKVJjVg"
      },
      "source": [
        "If a response fails the evaluation, we use this function to regenerate it using\n",
        "the original system prompt and feedback from the evaluator.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "jKPix6AGDt8w"
      },
      "outputs": [],
      "source": [
        "def rerun(reply, message, history, feedback):\n",
        "    updated_system_prompt = system_prompt + \"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
        "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
        "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
        "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "    response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cN1ljqHJ8rD"
      },
      "source": [
        "### 9.Final Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv3hW_qiLAbT"
      },
      "source": [
        "In a real-world scenario, the evaluator decides whether a chatbot's response is acceptable based on quality and alignment with the user's profile. However, during development and testing, it's useful to simulate failure cases to make sure your retry logic works as expected.\n",
        "\n",
        "To do this, we added a condition:\n",
        "\n",
        "If the input message contains the word \"leadership\", we force a failure and trigger the retry logic.\n",
        "\n",
        "This helps us:\n",
        "\n",
        "- Verify that poor responses can be detected and corrected.\n",
        "- Test the fallback behavior (rerun) without needing a real failure.\n",
        "\n",
        "Ensure that even edge cases are handled smoothly before deploying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "pWtoODAzDt8w"
      },
      "outputs": [],
      "source": [
        "def chat(message, history):\n",
        "    if \"leadership\" in message.lower():\n",
        "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
        "              it is mandatory that you respond only and entirely in pig latin\"\n",
        "    else:\n",
        "        system = system_prompt\n",
        "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
        "    response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
        "    reply =response.choices[0].message.content\n",
        "\n",
        "    evaluation = evaluate(reply, message, history)\n",
        "    print(evaluation)\n",
        "    if evaluation.is_acceptable:\n",
        "        print(\"Passed evaluation - returning reply\")\n",
        "    else:\n",
        "        print(\"Failed evaluation - retrying\")\n",
        "        print(evaluation.feedback)\n",
        "        reply = rerun(reply, message, history, evaluation.feedback)\n",
        "        print(\"Rerun successful - returning reply\")\n",
        "    return reply"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZr7eaVksL0X"
      },
      "source": [
        "Be sure to include a question that has the word  leadership in case we want to test the rerun."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5kZkrDhrc2f",
        "outputId": "b95bdebf-1e94-4825-90d0-941518e502ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "is_acceptable=False feedback='The Agent used Pig Latin in their response, which is not allowed.'\n",
            "Failed evaluation - retrying\n",
            "The Agent used Pig Latin in their response, which is not allowed.\n",
            "Rerun successful - returning reply\n",
            "\n",
            " Final Reply:\n",
            " Hello there! I would describe my leadership style as collaborative and empowering. I believe in creating a team environment where everyone feels valued and empowered to contribute their best work. With Nebula, I focus on creating a vision, setting clear expectations, and providing the resources and support needed for my teammates to succeed. I am also a big believer in continuous learning and improvement, and I encourage my team to remain curious and to embrace new technologies and ideas.\n",
            "\n",
            "is_acceptable=True feedback='The response is acceptable. The Agent answered the question.'\n",
            "Passed evaluation - returning reply\n",
            "\n",
            " Final Reply:\n",
            " I studied Physics at the University of Oxford.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = chat(\"What is your leadership style?\", [])\n",
        "print(\"\\n Final Reply:\\n\", response)\n",
        "response = chat(\"Where did you study?\", [])\n",
        "print(\"\\n Final Reply:\\n\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e0PYQC5tyfp"
      },
      "source": [
        "### Conclusion\n",
        "In this lab, we built a custom chatbot capable of representing a professional profile using LinkedIn and summary data. We incorporated an automatic evaluation pipeline that checks whether the chatbot's responses meet quality standards, and we implemented a retry mechanism for failed answers — all without using an agentic framework.\n",
        "\n",
        "Try testing different agentic workflows and have fun!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
