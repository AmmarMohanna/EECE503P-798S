{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning Agent Navigation in a Grid World\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this notebook, we'll simulate a simple **cleaning agent** that navigates a 2D grid environment to reach a goal while avoiding obstacles (dirt spots). The agent can move up, down, left, or right. The environment tracks its state and rewards the agent for reaching the goal.\n",
        "\n",
        "This example covers:\n",
        "\n",
        "- Environment modeling\n",
        "- State validation\n",
        "- Agent actions and policy\n",
        "- Visualization using emojis in the console\n",
        "\n",
        "The notebook is based on: https://towardsdatascience.com/reinforcement-learning-101-building-a-rl-agent-0431984ba178/\n"
      ],
      "metadata": {
        "id": "VXKql5zX9bNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "TyLzye2z9i0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Environment Setup: The GridWorld Class\n",
        "\n",
        "The `GridWorld` class models the environment:\n",
        "\n",
        "- It defines a grid size (`width` Ã— `height`).\n",
        "- Tracks the agent's start and goal positions.\n",
        "- Marks obstacles (dirt spots) the agent must avoid.\n",
        "- Supports resetting, checking valid states, and taking action steps.\n"
      ],
      "metadata": {
        "id": "7pz7snSK-KPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorld:\n",
        "    \"\"\"\n",
        "    GridWorld environment for navigation.\n",
        "\n",
        "    Args:\n",
        "    - width: Width of the grid\n",
        "    - height: Height of the grid\n",
        "    - start: Start position of the agent (tuple)\n",
        "    - goal: Goal position of the agent (tuple)\n",
        "    - obstacles: List of obstacle positions (list of tuples)\n",
        "\n",
        "    Methods:\n",
        "    - reset: Reset environment to start state\n",
        "    - is_valid_state: Check if a state is inside bounds and not blocked by an obstacle\n",
        "    - step: Move the agent in the environment based on an action\n",
        "    \"\"\"\n",
        "    def __init__(self, width: int = 5, height: int = 5, start: tuple = (0, 0), goal: tuple = (4, 4), obstacles: list = None):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.start = np.array(start)\n",
        "        self.goal = np.array(goal)\n",
        "        self.obstacles = [np.array(obstacle) for obstacle in obstacles] if obstacles else []\n",
        "        self.state = self.start\n",
        "        self.actions = {\n",
        "            'up': np.array([-1, 0]),\n",
        "            'down': np.array([1, 0]),\n",
        "            'left': np.array([0, -1]),\n",
        "            'right': np.array([0, 1])\n",
        "        }\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Reset the environment to the start state \"\"\"\n",
        "        self.state = self.start\n",
        "        return self.state\n",
        "\n",
        "    def is_valid_state(self, state):\n",
        "        \"\"\"\n",
        "        Check if a state is valid (inside grid and not an obstacle)\n",
        "        \"\"\"\n",
        "        inside_grid = (0 <= state[0] < self.height) and (0 <= state[1] < self.width)\n",
        "        not_obstacle = all((state != obstacle).any() for obstacle in self.obstacles)\n",
        "        return inside_grid and not_obstacle\n",
        "\n",
        "    def step(self, action: str):\n",
        "        \"\"\"\n",
        "        Take a step in the environment based on action.\n",
        "        Returns:\n",
        "        - next_state: the agent's new position\n",
        "        - reward: 100 if goal reached, else -1\n",
        "        - done: True if goal reached, else False\n",
        "        \"\"\"\n",
        "        next_state = self.state + self.actions[action]\n",
        "        if self.is_valid_state(next_state):\n",
        "            self.state = next_state\n",
        "        reward = 100 if (self.state == self.goal).all() else -1\n",
        "        done = (self.state == self.goal).all()\n",
        "        return self.state, reward, done\n"
      ],
      "metadata": {
        "id": "CSzSSe4c-NZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Defining the Agent's Navigation Policy\n",
        "\n",
        "The agent needs a way to decide which action to take at each step. Here, we use a simple heuristic policy:\n",
        "\n",
        "- From the current position, the agent checks possible moves.\n",
        "- It selects the valid move that minimizes the Manhattan distance to the goal.\n"
      ],
      "metadata": {
        "id": "bDfDMInS-Vkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def navigation_policy(state: np.array, goal: np.array, obstacles: list):\n",
        "    \"\"\"\n",
        "    Policy for the agent to navigate towards the goal.\n",
        "\n",
        "    Args:\n",
        "    - state: current position of the agent (np.array)\n",
        "    - goal: goal position (np.array)\n",
        "    - obstacles: list of obstacles (not used directly here but can be)\n",
        "\n",
        "    Returns:\n",
        "    - action (str): one of 'up', 'down', 'left', or 'right'\n",
        "    \"\"\"\n",
        "    actions = ['up', 'down', 'left', 'right']\n",
        "    valid_actions = {}\n",
        "\n",
        "    for action in actions:\n",
        "        next_state = state + env.actions[action]\n",
        "        if env.is_valid_state(next_state):\n",
        "            # Calculate Manhattan distance to goal\n",
        "            distance = np.sum(np.abs(next_state - goal))\n",
        "            valid_actions[action] = distance\n",
        "\n",
        "    # Choose the action that minimizes the distance to the goal\n",
        "    if valid_actions:\n",
        "        return min(valid_actions, key=valid_actions.get)\n",
        "    else:\n",
        "        return None  # No valid moves\n"
      ],
      "metadata": {
        "id": "Qh9p4snt-YFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Running the Simulation with Visualization\n",
        "\n",
        "We simulate the agent's moves step-by-step, printing the grid to the console using emojis:\n",
        "\n",
        "- `â¬œ` â€” Empty space\n",
        "- `â›”` â€” Obstacle (dirt)\n",
        "- `ðŸŸ«` â€” Goal (cleaning target)\n",
        "- `ðŸ¤–` â€” Agent (cleaning robot)\n",
        "\n",
        "The grid is printed at each step to show the agentâ€™s progress.\n"
      ],
      "metadata": {
        "id": "KpkXz3nU-aUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_simulation_with_policy(env: GridWorld, policy):\n",
        "    \"\"\"\n",
        "    Run the simulation until the agent reaches the goal or gets stuck.\n",
        "    Prints the grid with emojis at each step.\n",
        "    \"\"\"\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    logging.info(f\"Start State: {state}, Goal: {env.goal}, Obstacles: {env.obstacles}\")\n",
        "\n",
        "    while not done:\n",
        "        # Create grid filled with empty space emojis\n",
        "        grid_emoji = np.full((env.height, env.width), 'â¬œ')\n",
        "\n",
        "        # Place obstacles\n",
        "        for obstacle in env.obstacles:\n",
        "            grid_emoji[tuple(obstacle)] = 'â›”'\n",
        "\n",
        "        # Place goal\n",
        "        grid_emoji[tuple(env.goal)] = 'ðŸŸ«'\n",
        "\n",
        "        # Place agent\n",
        "        grid_emoji[tuple(state)] = 'ðŸ¤–'\n",
        "\n",
        "        # Print grid row by row\n",
        "        for row in grid_emoji:\n",
        "            print(' '.join(row))\n",
        "        print()  # Empty line between steps\n",
        "\n",
        "        action = policy(state, env.goal, env.obstacles)\n",
        "        if action is None:\n",
        "            logging.info(\"No valid actions available, agent is stuck.\")\n",
        "            break\n",
        "\n",
        "        next_state, reward, done = env.step(action)\n",
        "        logging.info(f\"State: {state} -> Action: {action} -> Next State: {next_state}, Reward: {reward}\")\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            logging.info(\"Goal reached!\")\n"
      ],
      "metadata": {
        "id": "7vWpoCh--bvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Setting Up and Running the Cleaning Agent\n",
        "\n",
        "Now let's define some dirt spots as obstacles and run our cleaning agent simulation:\n"
      ],
      "metadata": {
        "id": "FKYvyK0z-d9b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dirt spots (obstacles) on the grid\n",
        "obstacles = [(1, 1), (1, 2), (2, 1), (3, 3)]\n",
        "\n",
        "# Initialize the environment with obstacles\n",
        "env = GridWorld(obstacles=obstacles)\n",
        "\n",
        "# Run the simulation using the navigation policy\n",
        "run_simulation_with_policy(env, navigation_policy)\n"
      ],
      "metadata": {
        "id": "t_mTr4gP-faA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "- We modeled a grid environment with obstacles and a goal.\n",
        "- The cleaning agent moves step-by-step using a simple heuristic policy.\n",
        "- We visualized the agentâ€™s progress in the console with emojis.\n",
        "- This simple example can be extended to include more advanced policies, dynamic obstacles, or other features.\n",
        "\n",
        "Feel free to experiment by changing the grid size, obstacles, or policy!\n"
      ],
      "metadata": {
        "id": "kmMxoy43-iBS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "udMWfCpyyQl3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}